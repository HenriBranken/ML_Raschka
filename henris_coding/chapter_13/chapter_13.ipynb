{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 Parallelising Neural Network Training with TensorFlow\n",
    "\n",
    "In this chapter, we will start using TensorFlow and see how it brings significant benefits to training performance.\n",
    "\n",
    "In this chapter we will explore the following topics:\n",
    "* How TensorFlow improves training performance.\n",
    "* Working with TensorFlow to write optimised machine learning code.\n",
    "* Using TensorFlow high-level APIs to build a multilayer neural network.\n",
    "* Choosing activation functions for artificial neural networks.\n",
    "* Introducing Keras, a high-level wrapper around TensorFlow, for implementing common deep learning architectures most conveniently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is TensorFlow?\n",
    "\n",
    "TensorFlow is a scalable and multiplatform programming interface for implementing and running machine learning algorithms, including convenience wrappers for deep learning.\n",
    "\n",
    "Its greatest performance capabilities can be discovered when using GPUs.  TensorFlow supports CUDA-enabled GPUs officially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps with TensorFlow\n",
    "\n",
    "Tensorflow is built around a computation graph composed of a set of nodes.  Each node represents an operation that may have zero or more input or output.  The value that flow through the edges of the computation graph are called tensors.\n",
    "\n",
    "* A scalar is a rank-0 tensor.\n",
    "* A vector is a rank-1 tensor.\n",
    "* A matrix is a rank-2 tensor.\n",
    "* Matrices stacked in a third dimension are rank-3 tensors.\n",
    "\n",
    "Once a computation graph is built, the graph can be launched in a TensorFlow `Session` for executing different nodes of the graph.\n",
    "\n",
    "As a warm-up exercise, we will start with the use of simple scalars from TensorFlow to compte a net input $z$ of a sample point $x$ in a one-dimensional dataset with weight $w$, and bias $b$:\n",
    "\n",
    "$$\n",
    "z = w\\times x + b\n",
    "$$\n",
    "\n",
    "The following code shows the implementation of this equation in the low-level TensorFlow API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= 1.0 ---> z= 2.7\n",
      "x= 0.6 ---> z= 1.9\n",
      "x=-1.8 ---> z=-2.9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# create a graph\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=(None), name='x')\n",
    "    w = tf.Variable(2.0, name='weight')\n",
    "    b = tf.Variable(0.7, name='bias')\n",
    "    z = w*x + b\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "# create a session and pass in a graph g\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # Initialise w and b\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Evaluate z:\n",
    "    for t in [1.0, 0.6, -1.8]:\n",
    "        print(\"x={:4.1f} ---> z={:4.1f}\".format(t, sess.run(z, feed_dict={x:t})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, when we develop a model in the TensorFlow low-level API, we need to:\n",
    "* define placeholders for the input data (`x`, `y`, and sometimes other tunable parameters)\n",
    "* define the weight matrices,\n",
    "* and finally build the model from input to output.\n",
    "\n",
    "If this is an optimisation problem, we should define the **loss or cost function** and determine which **optimisation algorithm** to use.\n",
    "\n",
    "In the above code snippet, we created a placeholder for `x` with `shape=(None)`.  This allows us to feed the values in an\n",
    "* element-by-element form,\n",
    "* and as a batch of input data at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.7 4.7 6.7]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(z, feed_dict={x:[1.0, 2.0, 3.0]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with array structures\n",
    "\n",
    "By executing the following code, we will create a simple rank-3 tensor of size `batchsize x 2 x 3`, reshape it, and calculate the column sums using TensorFlow's optimised expressions.\n",
    "\n",
    "Since we do not know the batch size a priori, we specify `None` for the batch size in the argument for the `shape` parameter of the placeholder `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (3, 2, 3)\n",
      "Reshaped:\n",
      " [[ 0.  1.  2.  3.  4.  5.]\n",
      " [ 6.  7.  8.  9. 10. 11.]\n",
      " [12. 13. 14. 15. 16. 17.]]\n",
      "Column Sums:\n",
      " [18. 21. 24. 27. 30. 33.]\n",
      "Column Means:\n",
      " [ 6.  7.  8.  9. 10. 11.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Create a Graph\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=(None, 2, 3), name='input_x')\n",
    "    x2 = tf.reshape(x, shape=(-1, 6), name='x2')\n",
    "    \n",
    "    # Calculate the sum of each column\n",
    "    xsum = tf.reduce_sum(x2, axis=0, name='col_sum')\n",
    "    \n",
    "    # Calculate the mean of each column\n",
    "    xmean = tf.reduce_mean(x2, axis=0, name='col_mean')\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    x_array = np.arange(18).reshape(3, 2, 3)\n",
    "    print('Input shape: {}'.format(x_array.shape))\n",
    "    print('Reshaped:\\n', sess.run(x2, feed_dict={x:x_array}))\n",
    "    print('Column Sums:\\n', sess.run(xsum, feed_dict={x:x_array}))\n",
    "    print('Column Means:\\n', sess.run(xmean, feed_dict={x:x_array}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in this example, we worked with 3 functions:\n",
    "* `tf.reshape`\n",
    "* `tf.reduce_sum`\n",
    "* `tf.reduce_mean`\n",
    "For reshaping, we used the value `-1` for the first dimension.  This is because we do not know the value of the batch size.  When reshaping a tensor, if you use `-1` for a specific dimension, the size of that dimension will be computed according to the total size of the tensor and the remaining dimension.  Therefore, `tf.reshape(tensor, shape=(-1,))` can be used to flatten a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a simple model with the low-level TensorFlow API\n",
    "\n",
    "Let us now implement **Ordinary Least Squares (OLS) regression**.\n",
    "\n",
    "Let us start by creating a small one-dimensional toy dataset with `10` training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.arange(10).reshape((10, 1))\n",
    "y_train = np.array([1.0, 1.3, 3.1, 2.0, 5.0, 6.3, 6.6, 7.4, 8.0, 9.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, we need 2 placeholders, one for the input `x`, and one for the `y` for feeding the data into our model.  Next, we need to define the trainable variables: the weight `w`, and the bias `b`.\n",
    "\n",
    "Then we define the linear regression model as $z = wx + b$, followed by defining the cost function to be the **Mean of Squared Error (MSE)**.  To earn the weight parameters of the model, we use the gradient descent optimizer.  The code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfLinreg(object):\n",
    "    \n",
    "    def __init__(self, x_dim, learning_rate=0.1, random_seed=None):\n",
    "        self.x_dim = x_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.g = tf.Graph()\n",
    "        \n",
    "        # Build the model\n",
    "        with self.g.as_default():\n",
    "            # Set the graph-level random-seed\n",
    "            tf.set_random_seed(random_seed)\n",
    "            \n",
    "            self.build()\n",
    "            \n",
    "            # create the initialiser\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    def build(self):\n",
    "        # define the placeholders for the input\n",
    "        self.X = tf.placeholder(dtype=tf.float32, shape=(None, self.x_dim), name='x_input') # we don't know how many rows\n",
    "        self.y = tf.placeholder(dtype=tf.float32, shape=(None), name='y_input')\n",
    "        print(self.X)\n",
    "        print(self.y)\n",
    "        \n",
    "        # define the weight matrix and the bias vector\n",
    "        w = tf.Variable(tf.zeros(shape=(1)), name=\"weight\")\n",
    "        b = tf.Variable(tf.zeros(shape=(1)), name=\"bias\")\n",
    "        print(w)\n",
    "        print(b)\n",
    "        \n",
    "        self.z_net = tf.squeeze(w*self.X + b, name=\"z_net\")\n",
    "        print(self.z_net)\n",
    "        \n",
    "        sqr_errors = tf.square(self.y - self.z_net, name=\"sqr_errors\")\n",
    "        print(sqr_errors)\n",
    "        self.mean_cost = tf.reduce_mean(sqr_errors, name=\"mean_cost\")\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate, name='GradientDescent')\n",
    "        self.optimizer = optimizer.minimize(self.mean_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have defined a class to construct our model.  We will create an instance of this class and call it `lrmodel`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"x_input:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"y_input:0\", dtype=float32)\n",
      "<tf.Variable 'weight:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'bias:0' shape=(1,) dtype=float32_ref>\n",
      "Tensor(\"z_net:0\", dtype=float32)\n",
      "Tensor(\"sqr_errors:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "lrmodel = TfLinreg(x_dim=X_train.shape[1], learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `print` statemnts that we wrote in the `build` method will display information about the six nodes in the graph:\n",
    "* `X`\n",
    "* `y`\n",
    "* `w`\n",
    "* `b`\n",
    "* `z_net`\n",
    "* `sqr_errors`\n",
    "with their names and their shapes.\n",
    "\n",
    "For training, we implement a separate function that needs a\n",
    "1. TensorFlow session\n",
    "2. A model instance\n",
    "3. Training data\n",
    "4. The number of epochs as input arguments.\n",
    "\n",
    "In this function, first we **initialise** the variables in the TensorFlow session using the `init_op` operation defined in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linreg(sess, model, X_train, y_train, num_epochs=10):\n",
    "    # initialise all the variables: w and b\n",
    "    sess.run(model.init_op)\n",
    "    \n",
    "    training_costs = []\n",
    "    for i in range(num_epochs):\n",
    "        _, cost = sess.run([model.optimizer, model.mean_cost], feed_dict={model.X:X_train, model.y:y_train})\n",
    "        training_costs.append(cost)\n",
    "    \n",
    "    return training_costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we can create a new TensorFlow session to \n",
    "1. launch the `lrmodel.g` graph\n",
    "2. and pass all the required arguments to the `train_linreg` function for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=lrmodel.g) # launch the `lrmodel.g` graph\n",
    "training_costs = train_linreg(sess, lrmodel, X_train, y_train) # passing all the required arguments to the `train_linreg` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now visualise the training costs after these 10 epochs to see whether the mmodel is converging or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAAEmCAYAAAAOb7UzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XuYXHWd5/H3t6pvSadTRZIOJOkKDRIJkFgNtICAPiqijjfAdVYZRXZkiBcccWR1HGdmR11dcdbBHSXKRC7CiLiKcUHFC5thRUCJHegmN0wIJqRz7SR00umkk7589486HTpJXypNnzpVpz6v56mnTp06deqbUvLJ75zfxdwdERGRuEhEXYCIiMhEUrCJiEisKNhERCRWFGwiIhIrCjYREYkVBZuIiMSKgk1ERGJFwSYiIrGiYBMRkVipiLqAfMyYMcMbGxujLkNERCK0YsWKXe5eP9ZxJRFsjY2NtLS0RF2GiIhEyMw25XOcLkWKiEisKNhERCRWFGwiIhIrCjYREYkVBZuIiMSKgk1ERGJFwSYiIrGiYBMRkVgpi2B7bud+Ft3Twh+3d0VdioiIhKwsgq26IsGv1+ygZdOeqEsREZGQlUWwNZw0iWm1VbRt7oy6FBERCVlZBJuZkW1I0bZ5b9SliIhIyMoi2ACymTTrdnax/1Bf1KWIiEiIyirY3GHVFrXaRETirHyCrSENoPtsIiIxVzbBNq22irnTJtOqYBMRibWyCTaApkxaLTYRkZgrq2DLZtJs3dvDzn09UZciIiIhKatga8qkAGhrVwcSEZG4KqtgO2d2imTCdDlSRCTGyirYaiqTzD+ljrZ2BZuISFyVVbBB7j5b2+ZOBgY86lJERCQEZRdsTZk0+3r6+NPu7qhLERGREIQWbGZWY2bLzazNzFab2ReC/aeZ2ZNmtt7M/reZVYVVw3CaMhqoLSISZ2G22A4Bb3T3LNAEvNXMLgK+Cnzd3ecBLwLXhVjDcV5RP4XaqqSCTUQkpkILNs/ZH7ysDB4OvBG4P9h/N3BlWDUMJ5kwFjakaFWXfxGRWAr1HpuZJc2sFdgJPAxsADrdfXCK/XZgzgifXWRmLWbW0tHRMaF1ZTNp1m7dx6G+/gk9r4iIRC/UYHP3fndvAhqAC4CzhjtshM8ucfdmd2+ur6+f0LqaGtIc7h/g2W1dE3peERGJXkF6Rbp7J/D/gIuAtJlVBG81AFsLUcNQ2cEOJBrPJiISO2H2iqw3s3SwPQl4E7AWeAR4T3DYtcADYdUwklmpGmbWVdP6goJNRCRuKsY+ZNxmAXebWZJcgP7Q3X9mZmuAH5jZl4CngTtCrGFYZkY2k6ZVLTYRkdgJLdjc/Rng3GH2P0/uflukmjJpHl6zg70He0lNqoy6HBERmSBlN/PIoMEVtVeq27+ISKyUbbAtbBhcwkaXI0VE4qRsgy01qZLT62tp1QwkIiKxUrbBBrnxbK2bO3HXTP8iInFR3sE2N01H1yG27e2JuhQREZkgZR1sgx1INCGyiEh8lHWwzZ9VR1UyofFsIiIxUtbBVl2R5KzZU9ViExGJkbIONoCmhhQr2/fSP6AOJCIicVD2wZbNpOk+3M+Gjv1jHywiIkWv7IOtKZjpXxMii4jEQ9kHW+P0WqbWVKgDiYhITJR9sCUSuZn+1YFERCQeyj7YIDee7dntXfT09kddioiIvEwKNnIdSPoHnNVbNdO/iEipU7AB2WCm/9bNCjYRkVKnYANmTq1hdqpG99lERGJAwRZompvWEjYiIjGgYAtkG9K8sOcAe7oPR12KiIi8DAq2QDYYqK0VtUVESpuCLbBwToqEaQkbEZFSp2AL1FZXMG9mnYJNRKTEKdiGyGZStLXvxV0z/YuIlCoF2xDZTJo93YfZvOdg1KWIiMg4KdiGODLTvzqQiIiULAXbEK88uY6ayoTus4mIlLDQgs3MMmb2iJmtNbPVZnZjsP/zZrbFzFqDx9vCquFEVSYTLJidUrCJiJSwihDP3Qfc5O5PmVkdsMLMHg7e+7q7fy3E7x63bCbNvU9uord/gMqkGrQiIqUmtL+53X2buz8VbHcBa4E5YX3fRMlm0vT0DrBuR1fUpYiIyDgUpEliZo3AucCTwa6Pm9kzZnanmZ00wmcWmVmLmbV0dHQUokwAmhqCGUg007+ISEkKPdjMbArwY+CT7r4P+DbwCqAJ2Ab8y3Cfc/cl7t7s7s319fVhl3lEZtokptVW0br5xYJ9p4iITJxQg83MKsmF2r3uvhTA3Xe4e7+7DwDfAS4Is4YTZWZkG1JqsYmIlKgwe0UacAew1t1vGbJ/1pDDrgJWhVXDeGUzadbt7GL/ob6oSxERkRMUZq/IS4BrgJVm1hrs+xxwtZk1AQ5sBD4cYg3jks2kcYdVW/Zy0enToy5HREROQGjB5u6PATbMWw+F9Z0TJXukA0mngk1EpMRooNYwptVWMXfaZK3NJiJSghRsI8hm0upAIiJSghRsI2jKpNnSeZCdXT1RlyIiIidAwTaCpkwK0EBtEZFSo2AbwTmzUyQTpgmRRURKjIJtBDWVSeafUqcOJCIiJUbBNopcB5JOBgY86lJERCRPCrZRNDWk2dfTx8bd3VGXIiIieVKwjSKbyQ3UbtV9NhGRkqFgG8UZM6dQW5VUBxIRkRKiYBtFMmEsbEjR2q4u/yIipULBNoZsJs3arfs41NcfdSkiIpIHBdsYmhrSHO4f4NltXVGXIiIieVCwjWGwA4nGs4mIlAYF2xhmpWqor6tWz0gRkRKhYBuDmdGUSSvYRERKhIItD02ZNM93dLP3YG/UpYiIyBgUbHkYXFF7pbr9i4gUPQVbHhY2BEvYqAOJiEjRU7DlITWpktPra3WfTUSkBCjY8tTUkOtA4q6Z/kVEipmCLU/ZTJqOrkNs39cTdSkiIjKKMYPNzCry2Rd3TYMz/b+gy5EiIsUsnxbb8jz3xdr8WXVUJRO0qgOJiEhRG7HlZWYzgVnAJDNbCFjw1lRgcgFqKyrVFUnOmj1VS9iIiBS50S4pvh34ENAALOalYOsC/jHkuopSU0OK+1e00z/gJBM29gdERKTgRrwU6e53uftrgevc/XXu/trg8TZ3/9FYJzazjJk9YmZrzWy1md0Y7J9mZg+b2frg+aQJ/POEKptJ0324nw0d+6MuRURERpDPPbaZZjYVwMxuM7PlZnZZHp/rA25y97OAi4AbzOxs4LPAMnefBywLXpeEwZn+NZ5NRKR45RNsi9x9n5m9mdxlyY8C/zzWh9x9m7s/FWx3AWuBOcAVwN3BYXcDV46n8CicNr2WupoKBZuISBHLJ9gGRyT/GXCXu6/I83NHmFkjcC7wJHCyu2+DXPgBM0f4zCIzazGzlo6OjhP5utAkErmZ/tWBRESkeOUTUG1m9hDwTuAXZjaFl8JuTMHxPwY+6e778v2cuy9x92Z3b66vr8/3Y6HLNqR5dnsXPb39UZciIiLDyCfY/hL4PHCBux8AaoDr8jm5mVWSC7V73X1psHuHmc0K3p8F7DzRoqOUzaTpH3BWb9VM/yIixWjMYHP3fmAG8Bkzuxl4tbs/PdbnzMyAO4C17n7LkLceBK4Ntq8FHjjhqiOUDWb6b92sYBMRKUb5TKn1ZeAzwPPB49Nm9qU8zn0JcA3wRjNrDR5vA24GLjez9cDlweuSMXNqDbNTNbrPJiJSpPKZ8/GdwHnu3gdgZncCTwH/MNqH3P0xXhrUfax8hgsUrWwmrbXZRESKVL69G+tG2C5LTZk0m3YfYE/34ahLERGRY+QTbP8MPGVmt5vZHUAL8NVwyypugwO11WoTESk++XQe+R5wKfBQ8Hidu98bdmHFbOGcFAlD99lERIrQaLP7Xw7UuftSd98CLA32/4WZ7XD3ZYUqstjUVlcwb2adgk1EpAiN1mL7IvD4MPsfAf57OOWUjmwmRVv7XtzzHqsuIiIFMFqw1br7jmN3BtNg1YZXUmnIZtLs6T5M+4sHoy5FRESGGC3YJplZ8tidZlZBGS40eqxsg2b6FxEpRqMF21Lg38xs0uCOYHsx8H/CLqzYnXlKHTWVCQWbiEiRGS3Y/h7oBF4wsyfN7ElgE7kVtD9XiOKKWWUywYLZKXUgEREpMiP2igxmGvmvZvZ5YF6we727a/noQDaT5t4nN9HbP0Bl8oRW8hERkZDkM45tv7s/HTwUakNkM2l6egdYt6Mr6lJERCSgZsbL0BR0IGnTTP8iIkVDwfYyZKZN4qTJlbrPJiJSRMac3d/MXjXM7r3AZncfmPiSSoeZkc2k1TNSRKSI5LNszR1AE7Ca3DI0ZwGrgJSZLSrnqbUgN9P/b9atZ/+hPqZU5/NziohImPK5FLkeON/dm9w9C5wPtAJvAf4lzOJKQTaTxh1WbdF9NhGRYpBPsJ3l7s8MvnD3leQWHn0uvLJKR/ZIBxJdjhQRKQb5XDvbYGbfBH4QvH4v8JyZVQN9oVVWIqbVVjF32mStzSYiUiTyabF9EGgHPgv8HbAVuJZcqF0WXmmlI5tJq8u/iEiRGLPF5u4HyK2YPdyq2frbHMg2pPhp21Z2dvUws64m6nJERMramC02M7vIzH5hZmvMbN3goxDFlYpz52qgtohIscjnHttdwGeAFUB/uOWUpnNmp0gmjLbNnVx+9slRlyMiUtbyCbZ97v7T0CspYTWVSeafUqcOJCIiRSCfYPsPM/sKufXZDg3uHDoEQHIdSH7WtpWBASeRsKjLEREpW/kE26XHPAM48LqJL6d0NTWk+f6TL7Bxdzen10+JuhwRkbKVT6/I1xaikFKXzQQdSNo7FWwiIhEaMdjM7Gp3v8/MPjHc++7+jdFObGZ3Au8Adrr7gmDf54HrgY7gsM+5+0PjKbzYnDFzCrVVSdo27+WqcxuiLkdEpGyN1mI7KXiuH+e5vwvcCtxzzP6vu/vXxnnOopVMGAsbUjytqbVERCI1YrC5+7eC538cz4nd/VEzaxxfWaUpm0lz12MbOdTXT3VFMupyRETKUj7rsc0APgQ0Dj3e3ReN8zs/bmYfBFqAm9z9xRG+dxGwCGDu3Lnj/KrCampIc7h/gGe3dR255yYiIoWVz1yRDwAnA48By4Y8xuPbwCvIre+2jVGWvXH3Je7e7O7N9fXjvRpaWEM7kIiISDTy6e5f6+43TcSXufuOwW0z+w7ws4k4b7GYlaqhvq6a1s2dfPA1UVcjIlKe8mmx/cLM3jwRX2Zms4a8vIrcStyxYWZkG9Jam01EJEL5tNg+AvytmR0ADgMGuLtPG+1DZnYf8Hpghpm1A/8EvN7MmsgN8N4IfHj8pRenc+em+b9rd7D3YC+pSZVRlyMiUnbyCbYZ4zmxu189zO47xnOuUjK4ovbK9r1cOm9cP52IiLwMow3Qnufu64FzRjhEc0UOY2FDCsh1IFGwiYgU3mgtts8C1wGLh3lPc0WOIDWpktPra2nVfTYRkUiMNkD7uuBZc0WeoKaGNL99bhfujplm+hcRKaR87rFhZvOBs4GawX3u/v2wiip12UyapU9vYfu+HmalJkVdjohIWcln5pF/AN4MzAd+BbyF3GBtBdsIjgzU3typYBMRKbB8xrG9F3gDsM3drwGy5NnSK1dnzaqjKpnQhMgiIhHIJ9gOuns/0GdmdcB24PRwyypt1RVJzpo9VQO1RUQikE+wPW1maeBOchMXLweeCrWqGGhqSLGyfS/9Ax51KSIiZWXUYLNcl77Pu3unuy8G3g582N0/WJDqSlg2k6b7cD8bOvZHXYqISFkZNdjc3RkyUbG7P+fuaq3lYbADicaziYgUVj6XIpeb2XmhVxIzp02vpa6mQvfZREQKbLQptSrcvQ+4FLjezDYA3bw0CbLCbhSJRDDTv9ZmExEpqNG67S8HzgOuLFAtsdOUSXPbbzbQ09tPTWUy6nJERMrCaMFmAO6+oUC1xE42k6ZvwFm9dS/nnzrqKj8iIjJBRgu2ejP71EhvuvstIdQTK9lgpv/WzQo2EZFCGS3YksAUgpabnLiZU2uYnapRBxIRkQIaLdi2ufsXC1ZJTGUz6kAiIlJIo3X3V0ttAmQzaTbtPsCL3YejLkVEpCyMFmyXFayKGGsaHKitVpuISEGMGGzuvqeQhcTVwjkpEobus4mIFEg+M4/Iy1BbXcG8mXUKNhGRAlGwFUA2k6KtfS+5qTdFRCRMCrYCyGbS7Ok+TPuLB6MuRUQk9hRsBZBt0Ez/IiKFomArgDNPqaO6IqH7bCIiBaBgK4DKZIKFc1JqsYmIFEBowWZmd5rZTjNbNWTfNDN72MzWB88nhfX9xSabSbNq6156+weiLkVEJNbCbLF9F3jrMfs+Cyxz93nAsuB1Wchm0vT0DrBuR1fUpYiIxFpowebujwLHDvK+Arg72L6bMlrrrSnoQNK2eW/ElYiIxFuh77Gd7O7bAILnmSMdaGaLzKzFzFo6OjoKVmBYMtMmcdLkSnUgEREJWdF2HnH3Je7e7O7N9fX1UZfzspmZZvoXESmAQgfbDjObBRA87yzw90cq25Bm3Y4uug/1RV2KiEhsFTrYHgSuDbavBR4o8PdHqmlumgGHlVt0n01EJCxhdve/D/gdcKaZtZvZdcDNwOVmth64PHhdNrJHOpDocqSISFhGW0H7ZXH3q0d4q2zXeZtWW8XcaZN1n01EJERF23kkrrKZtLr8i4iESMFWYNmGFFs6D7KzqyfqUkREYknBVmBNmdx9tmfUahMRCYWCrcAWzEmRTJgmRBYRCYmCrcBqKpPMP6VOHUhEREKiYItArgNJJwMDHnUpIiKxo2CLQFNDmn09fWzc3R11KSIisaNgi0A26ECiy5EiIhNPwRaBM2ZOYXJVUuPZRERCoGCLQDJhLJyTUs9IEZEQKNgi0jQ3zZqt+zjU1x91KSIisaJgi0hTQ5rD/QM8u60r6lJERGJFwRYRdSAREQmHgi0is1I11NdV6z6biMgEU7BFxMzINqS1NpuIyARTsEWoKZNiQ0c3+3p6oy5FRCQ2FGwRGrzPtrJd49lERCaKgi1Cr2rIBZvus4mITBwFW4RSkyo5vb5WwSYiMoEUbBFrakjTurkTd830LyIyERRsEctm0nR0HWL7vp6oSxERiQUFW8QGO5D8ctX2iCsREYkHBVvEFsyeyqsbT+ILP13DLQ+v0+KjIiIvk4ItYhXJBN/7qwt5z/kNfGPZej567wq6D/VFXZaISMlSsBWB6ook//M9r+If33E2D6/Zwbu/9QQv7D4QdVkiIiVJwVYkzIzrLj2Nuz90Adv39fCuxY/xxIZdUZclIlJyIgk2M9toZivNrNXMWqKooVi9dl49D9xwCTOmVHPNHcu553cbNRRAROQERNlie4O7N7l7c4Q1FKXGGbX85GMX84Yz6/lvD6zmcz9ZyeG+gajLEhEpCboUWaTqaipZck0zN7zhFdy3fDPvv/337Np/KOqyRESKXlTB5sCvzWyFmS0a7gAzW2RmLWbW0tHRUeDyikMiYXz6LfP55tXnsnLLXt71zcdYtUUTJouIjCaqYLvE3c8D/gy4wcxed+wB7r7E3Zvdvbm+vr7wFRaRd2Znc/9HLgbgPbc9wU/btkZckYhI8Yok2Nx9a/C8E/gJcEEUdZSSBXNSPPDxS1kwO8Vf3/c0X/vVHzWYW0RkGAUPNjOrNbO6wW3gzcCqQtdRiurrqrn3+gt536sz3PrIcyz69xa6tEipiMhRomixnQw8ZmZtwHLg5+7+ywjqKEnVFUm+8u6FfOFd5/DIHzt497eeYOOu7qjLEhEpGgUPNnd/3t2zweMcd/9yoWsodWbGtRc38u8fuoCO/Ye4YvHjPLZeg7lFREDd/UvaxWfM4MEbLuWUqTVce9dy7nzsTxrMLSJlT8FW4uZOn8yPP3Yxl82fyRd/tobP3P8Mh/r6oy5LRCQyCrYYmFJdwW0fOJ9PXDaPH61o5+olv2dnlxYuFZHypGCLiUTC+NTlr+Rb7z+Ptdu6uOLWx3mmvTPqskRECk7BFjNvWziL+z/6GhJm/Pltv+OB1i1RlyQiUlAKthg6Z3aKBz9+CdlMmht/0MrNv3iWfg3mFpEyoWCLqelTqvnedRfy/gvncttvNnD9PS3s02BuESkDCrYYq6pI8OWrFvKlKxfw6LoOrlr8OM937I+6LBGRUCnYysAHLjqV7/3Vhbx4oJcrFz/Oo+vKc7UEESkPCrYycdHp03nghkuYnZ7Ef7lrObf/9nkN5haRWFKwlZHMtMn8+KMX85ZzTuFLP1/LTT9qo6dXg7lFJF4UbGWmtrqCxX9xHn/zpley9KktvG/J79mxT4O5RSQ+FGxlKJEwbnzTPG77wPms29HFu259jNbNGswtIvGgYCtjb11wCks/djFVFQn+87/9jqVPtUddkojIy6ZgK3PzT5nKAzdcyvlzT+JTP2zjfzy0VoO5RaSkKdiEabVV3HPdBVz7mlNZ8ujzfOi7f2DvQQ3mFpHSpGATACqTCb5wxQK+8u6FPLFhF1ctfpwNGswtIiVIwSZHufqCuXz/+ovYe7CXK299nEee3Rl1SSIiJ8RKYZBuc3Ozt7S0RF1GWdnSeZBF97SwZts+Xt04jdOm13LqjMk0Tq+lcXotp06fTG11RdRlikgZMbMV7t481nH6m0mGNSc9ifs/cjG3PPxHnn6hk2XP7mTX/kNHHVNfV03j9MmcOr2WxumTaZzxUujV1VRGVLmIlDsFm4xoUlWSv3/72Ude7z/Ux8Zd3WzafYCNu7vZtLubjbsP8Nv1Hdy/4ujQm15bxanTgxbejNqXtqfXkpqs0BOR8CjYJG9TqitYMCfFgjmp4947cLiPTbsPHAm7Tbu72bjrAL9/fjdLnz56sdP05MqXWnnTa2mcMdjqq+WkyZWYWaH+SCISQwo2mRCTqyo4a9ZUzpo19bj3enr7eWHPgWNaewdo2fgiD7ZtZeht3rqaiiOtvKGXOU+dXsuMKVUKPREZk4JNQldTmeSVJ9fxypPrjnvvUF8/m/ccPKql96dd3bRt7uTnz2xl6Fjx2qokp06v5bQhlzbTkyuprEhQnUxQWZGgMpmgMmlUH9lOUBVsVwXbyYTCUSTOFGwSqeqKJGfMnMIZM6cc997hvgG2dB7MtfB25YJv4+5u1mzbx69Wb6dvnDOkJIyjgq4ymaCywqgaEoSD25UVCaqS9tJxR71/zP6h50saFUnDMAYbmWaGAWaQGLJNcIwNOSaRACO30475rGEkjOC9Yz470vaQz75Uz9G/S+6oIa+Hyf9xfWaMcxx/xPDnGct4/rkynisAE/nPoom+AHHs/x7FZObUamoqkwX5LgWbFK2qigSnzci10Djz6Pf6+gfY2tnDvp5eDvcP0Ns3kHvuH+BwnwfPude9/QMc6hugt//o/S8dn3vv8DGf6e1zDhzspXfo8X0DHD7mPOMNWJFy8v3rL+TiV8woyHdFEmxm9lbgX4EkcLu73xxFHVK6KpIJ5k6fHHUZAAwM+JGQHBqefQOOu+MQ3Ed03DnyesAHX/uR+4xDXw+M8ll3ZyA4lqH7hxw34IPvBfuOHJc771DHD2c9PqyPPWbscwT1ncA5COorhPF8zfG/XGG/v5Dnm2jDXZUJS8GDzcySwGLgcqAd+IOZPejuawpdi8hESCSMmkSyYJdZRGR0UUypdQHwnLs/7+6HgR8AV0RQh4iIxFAUwTYH2DzkdXuw7yhmtsjMWsyspaOjo2DFiYhIaYsi2IbrtnP8FX73Je7e7O7N9fX1BShLRETiIIpgawcyQ143AFsjqENERGIoimD7AzDPzE4zsyrgfcCDEdQhIiIxVPBeke7eZ2YfB35Frrv/ne6+utB1iIhIPEUyjs3dHwIeiuK7RUQk3rSCtoiIxIqCTUREYsUKNX3Ny2FmHcCmqOsI2QxgV9RFlCD9buOj32189LuN30T8dqe6+5jjv0oi2MqBmbW4e3PUdZQa/W7jo99tfPS7jV8hfztdihQRkVhRsImISKwo2IrHkqgLKFH63cZHv9v46Hcbv4L9drrHJiIisaIWm4iIxIqCTUREYkXBFiEzy5jZI2a21sxWm9mNUddUSswsaWZPm9nPoq6llJhZ2szuN7Nng//vvSbqmkqBmf1N8N/pKjO7z8xqoq6pGJnZnWa208xWDdk3zcweNrP1wfNJYdagYItWH3CTu58FXATcYGZnR1xTKbkRWBt1ESXoX4Ffuvt8IIt+wzGZ2RzgE0Czuy8gN4H7+6Ktqmh9F3jrMfs+Cyxz93nAsuB1aBRsEXL3be7+VLDdRe4vmONWE5fjmVkD8Hbg9qhrKSVmNhV4HXAHgLsfdvfOaKsqGRXAJDOrACajdSSH5e6PAnuO2X0FcHewfTdwZZg1KNiKhJk1AucCT0ZbScn4X8BngIGoCykxpwMdwF3BZdzbzaw26qKKnbtvAb4GvABsA/a6+6+jraqknOzu2yD3D3pgZphfpmArAmY2Bfgx8El33xd1PcXOzN4B7HT3FVHXUoIqgPOAb7v7uUA3IV8WioPgntAVwGnAbKDWzD4QbVUyEgVbxMysklyo3evuS6Oup0RcArzLzDYCPwDeaGbfi7akktEOtLv74JWB+8kFnYzuTcCf3L3D3XuBpcDFEddUSnaY2SyA4HlnmF+mYIuQmRm5ex1r3f2WqOspFe7+d+7e4O6N5G7g/4e761/PeXD37cBmMzsz2HUZsCbCkkrFC8BFZjY5+O/2MtTp5kQ8CFwbbF8LPBDml0WygrYccQlwDbDSzFqDfZ8LVhgXCctfA/eaWRXwPPCXEddT9Nz9STO7H3iKXG/mp9H0WsMys/uA1wMzzKwd+CfgZuCHZnYduX8k/HmoNWhKLRERiRNdihQRkVhRsImISKwo2EREJFYUbCIiEisKNhERiRUFm0gBmVm/mbUOeUzYrB9m1jh0RnWRcqVxbCKFddDdm6IuQiTO1GITKQJmttHMvmpmy4PHGcH+U81smZk9EzzPDfafbGY/MbO24DE4vVPSzL4TrBv2azObFNkfSiQiCjaRwpp0zKXI9w55b5+7XwDcSm71AoLte9z9VcC9wDeC/d8AfuPuWXJzPa4O9s8DFrv7OUAn8J9C/vOIFB3NPCJSQGa2392nDLN/I/BGd38+mBh7u7v6Ihn2AAAA2klEQVRPN7NdwCx37w32b3P3GWbWATS4+6Eh52gEHg4Wc8TM/haodPcvhf8nEykearGJFA8fYXukY4ZzaMh2P7qPLmVIwSZSPN475Pl3wfYT5FYwAHg/8FiwvQz4KICZJYOVsUUE/WtOpNAmDVnJAeCX7j7Y5b/azJ4k9w/Oq4N9nwDuNLNPk1v5enAm/huBJcFs6f3kQm5b6NWLlADdYxMpAsE9tmZ33xV1LSKlTpciRUQkVtRiExGRWFGLTUREYkXBJiIisaJgExGRWFGwiYhIrCjYREQkVv4/sMQ32vPL/DMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, len(training_costs)+1), training_costs)\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, this simple model converges very quickly after a few epochs.\n",
    "\n",
    "Now, let us compile a new function to make predictions based on the input features.  For this function, we need the\n",
    "1. Tensorflow session,\n",
    "2. the model,\n",
    "3. and the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linreg(sess, model, X_test):\n",
    "    y_pred = sess.run(model.z_net, feed_dict={model.X:X_test})\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we only needed to run `z_net` defined in the graph computes the predicted output values.\n",
    "\n",
    "Next, let us plot the linear regression fit on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_train, y_train, marker='s', s=50, label='Training Data.')\n",
    "plt.plot(range(X_train.shape[0]), predict_linreg(sess, lrmodel, X_train), color='gray', marker='o',\n",
    "        markersize=6, linewidth=3, label='LinReg Model')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the resulting plot, our model fits the training data points appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building multilayer neural networks using TensorFlow's `Layers` API\n",
    "\n",
    "Let us implement a multilayer perceptron to classify the handwritten digits from the MNIST dataset.\n",
    "\n",
    "After downloading and unzipping the archives, we place the files in the `mnist` directory in our cwd so that we can load the training as well as the test dataset, using the `load_mnist(path, kind)` function we implemented previously in Chapter 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, \"{}-labels-idx1-ubyte\".format(kind))\n",
    "    images_path = os.path.join(path, \"{}-images-idx3-ubyte\".format(kind))\n",
    "    \n",
    "    with open(labels_path, \"rb\") as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "    \n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII', imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "        images = ((images / 255.) - 0.5) * 2 # Standardise the images to have the range [-1, +1]\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, Columns: 784.\n"
     ]
    }
   ],
   "source": [
    "# Load the Training data\n",
    "X_train, y_train = load_mnist('./mnist/', kind='train')\n",
    "print('Rows: {}, Columns: {}.'.format(X_train.shape[0], X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 10000, Columns: 784.\n"
     ]
    }
   ],
   "source": [
    "# Load the test data\n",
    "X_test, y_test = load_mnist('./mnist/', kind='t10k')\n",
    "print('Rows: {}, Columns: {}.'.format(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n"
     ]
    }
   ],
   "source": [
    "# Performing Mean centering and Normalisation\n",
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "X_train_centered = (X_train - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val\n",
    "\n",
    "del X_train, X_test # we don't need the un-normalised data anymore.\n",
    "\n",
    "print(X_train_centered.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_centered.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start building our model.  We will start by creating two placeholders, named `tf_x` and `tf_y`, and then build a multilayer perceptron, but with 3 fully connected layers.\n",
    "\n",
    "However, we will replace the logistic units in the hidden layer with **hyperbolic tangent** activation functions (`tanh`), replace the logistic function in the output layer with `softmax`, and **add an additional hidden layer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_features = X_train_centered.shape[1] # the number of columns (784)\n",
    "n_classes = 10 # 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
    "random_seed = 123\n",
    "np.random.seed(random_seed) # seed the generator\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():  # Returns a context manager that makes this Graph the default graph\n",
    "    tf.set_random_seed(random_seed)\n",
    "    tf_x = tf.placeholder(dtype=tf.float32, shape=(None, n_features), name='tf_x')  # rows/n_samples unknown\n",
    "    tf_y = tf.placeholder(dtype=tf.int32, shape=None, name='tf_y')  # rows/n_samples unknown\n",
    "    y_onehot = tf.one_hot(indices=tf_y, depth=n_classes)\n",
    "    \n",
    "    h1 = tf.layers.dense(inputs=tf_x, units=60, activation=tf.tanh, name='layer1')\n",
    "    h2 = tf.layers.dense(inputs=h1, units=60, activation=tf.tanh, name='layer2')  # add additional layer\n",
    "    logits = tf.layers.dense(inputs=h2, units=10, activation=None, name='layer3')  # output\n",
    "    predictions = {'classes': tf.argmax(logits, axis=1, name='predicted_classes'), \n",
    "                   'probabilities': tf.nn.softmax(logits, name='softmax_tensor')}  # output layer replaced with softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next on, we dfine the cost functions, and add an operator for initialising the model variables as well as an optimisation operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cost function and optimizer:\n",
    "with g.as_default():\n",
    "    cost = tf.losses.softmax_cross_entropy(onehot_labels=y_onehot, logits=logits)  # define cost function\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(loss=cost)\n",
    "    \n",
    "    # initialise the model variables\n",
    "    init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training the network, we need a way to generate batches of data.\n",
    "For this, we implement the following function that returns a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_generator(X, y, batch_size=128, shuffle=False):\n",
    "    X_copy = np.array(X)\n",
    "    y_copy = np.array(y)\n",
    "    \n",
    "    if shuffle:\n",
    "        data = np.column_stack((X_copy, y_copy))\n",
    "        np.random.shuffle(data)\n",
    "        X_copy = data[:, :-1]\n",
    "        y_copy = data[:, -1].astype(int)\n",
    "    \n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X_copy[i:i+batch_size, :], y_copy[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can,\n",
    "* create a new TensorFlow session,\n",
    "* initialize all the variables in our network,\n",
    "* and train it.\n",
    "\n",
    "We also display the average training loss after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Epoch   1  Average Training Loss: 1.4735.\n",
      " --- Epoch   2  Average Training Loss: 0.8826.\n",
      " --- Epoch   3  Average Training Loss: 0.7048.\n",
      " --- Epoch   4  Average Training Loss: 0.6064.\n",
      " --- Epoch   5  Average Training Loss: 0.5420.\n",
      " --- Epoch   6  Average Training Loss: 0.4963.\n",
      " --- Epoch   7  Average Training Loss: 0.4619.\n",
      " --- Epoch   8  Average Training Loss: 0.4349.\n",
      " --- Epoch   9  Average Training Loss: 0.4132.\n",
      " --- Epoch  10  Average Training Loss: 0.3952.\n",
      " --- Epoch  11  Average Training Loss: 0.3800.\n",
      " --- Epoch  12  Average Training Loss: 0.3669.\n",
      " --- Epoch  13  Average Training Loss: 0.3555.\n",
      " --- Epoch  14  Average Training Loss: 0.3454.\n",
      " --- Epoch  15  Average Training Loss: 0.3364.\n",
      " --- Epoch  16  Average Training Loss: 0.3283.\n",
      " --- Epoch  17  Average Training Loss: 0.3210.\n",
      " --- Epoch  18  Average Training Loss: 0.3143.\n",
      " --- Epoch  19  Average Training Loss: 0.3081.\n",
      " --- Epoch  20  Average Training Loss: 0.3024.\n",
      " --- Epoch  21  Average Training Loss: 0.2970.\n",
      " --- Epoch  22  Average Training Loss: 0.2920.\n",
      " --- Epoch  23  Average Training Loss: 0.2873.\n",
      " --- Epoch  24  Average Training Loss: 0.2829.\n",
      " --- Epoch  25  Average Training Loss: 0.2787.\n",
      " --- Epoch  26  Average Training Loss: 0.2748.\n",
      " --- Epoch  27  Average Training Loss: 0.2710.\n",
      " --- Epoch  28  Average Training Loss: 0.2674.\n",
      " --- Epoch  29  Average Training Loss: 0.2639.\n",
      " --- Epoch  30  Average Training Loss: 0.2606.\n",
      " --- Epoch  31  Average Training Loss: 0.2574.\n",
      " --- Epoch  32  Average Training Loss: 0.2543.\n",
      " --- Epoch  33  Average Training Loss: 0.2514.\n",
      " --- Epoch  34  Average Training Loss: 0.2485.\n",
      " --- Epoch  35  Average Training Loss: 0.2458.\n",
      " --- Epoch  36  Average Training Loss: 0.2431.\n",
      " --- Epoch  37  Average Training Loss: 0.2405.\n",
      " --- Epoch  38  Average Training Loss: 0.2380.\n",
      " --- Epoch  39  Average Training Loss: 0.2355.\n",
      " --- Epoch  40  Average Training Loss: 0.2332.\n",
      " --- Epoch  41  Average Training Loss: 0.2308.\n",
      " --- Epoch  42  Average Training Loss: 0.2286.\n",
      " --- Epoch  43  Average Training Loss: 0.2264.\n",
      " --- Epoch  44  Average Training Loss: 0.2243.\n",
      " --- Epoch  45  Average Training Loss: 0.2222.\n",
      " --- Epoch  46  Average Training Loss: 0.2201.\n",
      " --- Epoch  47  Average Training Loss: 0.2182.\n",
      " --- Epoch  48  Average Training Loss: 0.2162.\n",
      " --- Epoch  49  Average Training Loss: 0.2143.\n",
      " --- Epoch  50  Average Training Loss: 0.2124.\n"
     ]
    }
   ],
   "source": [
    "# create a session to launch the graph\n",
    "sess = tf.Session(graph=g)\n",
    "\n",
    "# run the variable initialisation operator\n",
    "sess.run(init_op)\n",
    "\n",
    "# 50 epochs of training\n",
    "for epoch in range(50):\n",
    "    training_costs = []\n",
    "    batch_generator = create_batch_generator(X_train_centered, y_train, batch_size=64)\n",
    "    for batch_X, batch_y in batch_generator:\n",
    "        # prepare a dictionary to feed data into our network:\n",
    "        feed = {tf_x: batch_X, tf_y: batch_y}\n",
    "        _, batch_cost = sess.run([train_op, cost], feed_dict=feed)\n",
    "        training_costs.append(batch_cost)\n",
    "    print(' --- Epoch {:3.0f}  Average Training Loss: {:.4f}.'.format(epoch + 1, np.mean(training_costs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the trained model to do predictions on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do prediction on the test set:\n",
    "feed = {tf_x: X_test_centered}\n",
    "y_pred = sess.run(predictions['classes'], feed_dict=feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will develop a similar classification model for MNIST using Keras, which is another high-level TensorFlow API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a multilayer neural network with Keras\n",
    "\n",
    "Keras allows us to utilize our GPUs to accelerate neural network training.  It has a very intuitive and user-friendly API, which allows us to implement neural networks in only a few lines of code.\n",
    "\n",
    "Firstly, we need to load the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, Columns: 784.\n",
      "Rows: 10000, Columns: 784.\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist('mnist/', kind='train')\n",
    "print('Rows: {:.0f}, Columns: {:.0f}.'.format(X_train.shape[0], X_train.shape[1]))\n",
    "\n",
    "X_test, y_test = load_mnist('mnist/', kind='t10k')\n",
    "print('Rows: {:.0f}, Columns: {:.0f}.'.format(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Mean centering, and normalisation\n",
    "mean_vals = np.mean(X_train, axis=0)  # for every pixel feature (784) there is a mean value\n",
    "std_val = np.std(X_train)\n",
    "\n",
    "X_train_centered = (X_train - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val\n",
    "\n",
    "del X_train, X_test  # we do not further need X_train and X_test\n",
    "\n",
    "print(X_train_centered.shape, y_train.shape)\n",
    "print(X_test_centered.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us set the random seed for NumPy and TensorFlow so that we get consistent results when rerunning the cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "np.random.seed(123)\n",
    "tf.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to convert the class labels (integers 0-9) into the one-hot format.  Fortunately, Keras provides a convenient tool for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first three labels:  [5 0 4].\n",
      "\n",
      "The first three labels one-hot encoded:\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]].\n"
     ]
    }
   ],
   "source": [
    "y_train_onehot = keras.utils.to_categorical(y_train)\n",
    "\n",
    "print(\"The first three labels:  {}.\".format(y_train[:3]))\n",
    "print(\"\\nThe first three labels one-hot encoded:\\n{}.\".format(y_train_onehot[:3, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now implement a neural network.  Briefly, we will have 3 layers:\n",
    "* First two layers each have 50 hidden units each with the `tanh` activation function.\n",
    "* The last layer has 10 layers for the 10 class labels; it also uses the `softmax` to give the probability of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        units=70,\n",
    "        input_dim=X_train_centered.shape[1],\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='tanh'))\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        units=70,\n",
    "        input_dim=70,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='tanh'))\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        units=y_train_onehot.shape[1],\n",
    "        input_dim=70,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='softmax'))\n",
    "\n",
    "sgd_optimizer = keras.optimizers.SGD(\n",
    "    lr=0.001, decay=1e-7, momentum=0.9)\n",
    "\n",
    "model.compile(optimizer=sgd_optimizer,\n",
    "             loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialise a new model using the `Sequential` class to implement a feedforward neural network.\n",
    "\n",
    "Since the first layer that we add is the input layer, we have to make sure that the `input_dim` attribute matches the number of features (columns) in the training set.\n",
    "\n",
    "Also, we have to make sure that the number of output units (`units`) and the input units (`input_dim`) of 2 consecutuve layers match.  In the preceding example, we added two hidden layers with 50 hidden units plus 1 bias unit each.\n",
    "\n",
    "The number of units in the output layer should be equal to the number of unique class labels---the number of columns in the one-hot-encoded class label array.\n",
    "\n",
    "Notice that we used a new initialisation algorithm for weight matrices by setting `kernel_initializer='glorot_uniform'`.  Glorot initialization is a more robust way of initialisation for deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can compile our model, we also have to define an optimizer.\n",
    "\n",
    "Furthermore, we can set values for the weight decay constant and momentum learning to adjust the learning rate at each epoch as discussed.\n",
    "\n",
    "Binary cross-entorpy is just a technical term for the cost function in the logistic regression, and categorical cross-entropy is its generalization for multiclass predictions via softmax.\n",
    "\n",
    "After compiling the model we can now train the model by calling the `fit` method.  We can also follow the optimization of the cost function during training by setting `verbose=1`.\n",
    "\n",
    "The `validation_split` parameter is handy since it will reserve x percent of the training data for validation after each epoch so that we can monitor whether the model is overfitting during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "54000/54000 [==============================] - 1s 27us/step - loss: 0.6525 - val_loss: 0.3260\n",
      "Epoch 2/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.3414 - val_loss: 0.2562\n",
      "Epoch 3/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.2854 - val_loss: 0.2243\n",
      "Epoch 4/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.2522 - val_loss: 0.2032\n",
      "Epoch 5/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.2287 - val_loss: 0.1875\n",
      "Epoch 6/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.2098 - val_loss: 0.1753\n",
      "Epoch 7/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.1944 - val_loss: 0.1661\n",
      "Epoch 8/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.1812 - val_loss: 0.1587\n",
      "Epoch 9/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.1697 - val_loss: 0.1517\n",
      "Epoch 10/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.1596 - val_loss: 0.1455\n",
      "Epoch 11/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.1507 - val_loss: 0.1403\n",
      "Epoch 12/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.1424 - val_loss: 0.1363\n",
      "Epoch 13/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.1351 - val_loss: 0.1322\n",
      "Epoch 14/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.1283 - val_loss: 0.1277\n",
      "Epoch 15/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.1222 - val_loss: 0.1253\n",
      "Epoch 16/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.1166 - val_loss: 0.1224\n",
      "Epoch 17/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.1112 - val_loss: 0.1203\n",
      "Epoch 18/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.1064 - val_loss: 0.1168\n",
      "Epoch 19/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.1018 - val_loss: 0.1152\n",
      "Epoch 20/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0975 - val_loss: 0.1142\n",
      "Epoch 21/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0935 - val_loss: 0.1124\n",
      "Epoch 22/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.0896 - val_loss: 0.1112\n",
      "Epoch 23/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.0861 - val_loss: 0.1094\n",
      "Epoch 24/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0826 - val_loss: 0.1087\n",
      "Epoch 25/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0795 - val_loss: 0.1079\n",
      "Epoch 26/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.0764 - val_loss: 0.1073\n",
      "Epoch 27/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0735 - val_loss: 0.1056\n",
      "Epoch 28/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0708 - val_loss: 0.1041\n",
      "Epoch 29/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.0681 - val_loss: 0.1035\n",
      "Epoch 30/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0656 - val_loss: 0.1032\n",
      "Epoch 31/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0633 - val_loss: 0.1029\n",
      "Epoch 32/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0609 - val_loss: 0.1024\n",
      "Epoch 33/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0587 - val_loss: 0.1023\n",
      "Epoch 34/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0566 - val_loss: 0.1013\n",
      "Epoch 35/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0546 - val_loss: 0.1009\n",
      "Epoch 36/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0525 - val_loss: 0.1002\n",
      "Epoch 37/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.0508 - val_loss: 0.1005\n",
      "Epoch 38/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.0490 - val_loss: 0.1001\n",
      "Epoch 39/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.0473 - val_loss: 0.0995\n",
      "Epoch 40/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0457 - val_loss: 0.0996\n",
      "Epoch 41/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.0440 - val_loss: 0.0992\n",
      "Epoch 42/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.0425 - val_loss: 0.0994\n",
      "Epoch 43/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0411 - val_loss: 0.0985\n",
      "Epoch 44/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0397 - val_loss: 0.0983\n",
      "Epoch 45/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.0384 - val_loss: 0.0983\n",
      "Epoch 46/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0370 - val_loss: 0.0986\n",
      "Epoch 47/50\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.0358 - val_loss: 0.0977\n",
      "Epoch 48/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0346 - val_loss: 0.0982\n",
      "Epoch 49/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0334 - val_loss: 0.0978\n",
      "Epoch 50/50\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.0323 - val_loss: 0.0975\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_centered, y_train_onehot, batch_size=64, epochs=50, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the value of the cost function is extremely useful during training.  This is because we can quickly spot whether the cost is decreasing during training and stop the algorithm earlier, if otherwise, to tune the hyperparameter values.\n",
    "\n",
    "To predict the class labels, we can use the `predict_classes` method to return the class labels directly as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 3 predictions:  [5 0 4].\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, verbose=0)\n",
    "print(\"The first 3 predictions:  {}.\".format(y_train_pred[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us pring the model accuracy on the training and the test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.32%.\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, verbose=0)\n",
    "correct_preds = np.sum(y_train == y_train_pred)\n",
    "train_acc = correct_preds / y_train.shape[0]\n",
    "print(\"Training Accuracy: {:.2f}%.\".format(train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.51%.\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict_classes(X_test_centered, verbose=0)\n",
    "correct_preds = np.sum(y_test == y_test_pred)\n",
    "test_acc = correct_preds / y_test.shape[0]\n",
    "print(\"Test Accuracy: {:.2f}%.\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Activation Functions for Multilayer Networks.\n",
    "\n",
    "A more precise definition for the sigmoid function would be the _logistic function or the negative log-likelihood function_.\n",
    "\n",
    "In practice it is not very useful to use linear activation functions for both hidden and output layers since we want to introduce nonlinearity in a typical artificial neural network to be able to tackle complex problems.\n",
    "\n",
    "The logistic activation function that we used in Chapter 12 mimics the concept of a neuron in a brain most closely---we can think of it as the probability of whether a neuron fires or not.\n",
    "\n",
    "However, logistic activation functions can be problematic if we have highly negative input since the output of the sigmoid function would be close to zero in this case.  If the sigmoid function returns output that are close to zero, the neural network would learn very slowly and it becomes more likely that it gets trapped in the local minima during training.\n",
    "\n",
    "Before we discuss what a hyperbolic tangent looks like, let's briefly recapitualte some of the basics of the logistic function and look at a generalisation that makes it more useful for multiplable classifaction problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function Recap\n",
    "\n",
    "We can use the logistic function to model the probability that a sample $x$ belongs to the positive class (class 1) in a binary classification task.  The given net input $z$ is shown in the following equation:\n",
    "\n",
    "$$\n",
    "z = w_0x_0 + w_1x_1 + \\ldots + w_mx_m = \\sum_{i=0}^{m}w_ix_i = \\mathbf{w}^T\\mathbf{x}\n",
    "$$\n",
    "Note that $w_0$ is the bias-unit (y-axis intercept, which means that $x_0 = 1$).\n",
    "\n",
    "The logistic function will be computed as follows:\n",
    "\n",
    "$$\n",
    "\\phi_{\\mathrm{logistic}}(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "To provide a more concrete example, let us assume a model for a two-dimensional data point $\\mathbf{x}$ and a model with the following weight coefficients assigned to the $\\mathbf{w}$ vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(y=1|x) = 0.888.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([1.0, 1.4, 2.5])  # the first value MUST BE 1\n",
    "w = np.array([0.4, 0.3, 0.5])\n",
    "\n",
    "def net_input(X, w):\n",
    "    return np.dot(X, w)\n",
    "\n",
    "def logistic(z):\n",
    "    return 1.0 / (1.0 + np.exp(-1.0*z))\n",
    "\n",
    "def logistic_activation(X, w):\n",
    "    z = net_input(X, w)\n",
    "    return logistic(z)\n",
    "\n",
    "print('P(y=1|x) = {:.3f}.'.format(logistic_activation(X, w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we calculate the net input and use it to activate a logistic neuron with those particular feature values and weight coefficients, then we get a value of `0.888`, which we can interpret as 88.8% probability that this particular sample $x$ belongs to the positive class.\n",
    "\n",
    "Next we demonstrate that an output layer consisting of multiple logistic activaiton units does not produce any meaningful, interpretable probability values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net Input:\n",
      "[1.78 0.76 1.65].\n",
      "\n",
      "Output Units:\n",
      "[0.85569687 0.68135373 0.83889105].\n"
     ]
    }
   ],
   "source": [
    "# 2d Weights array:  Array with the shape = (n_output_units, n_hidden_units+1)\n",
    "# Notice that the first column contains all the bias units.\n",
    "\n",
    "W = np.array([[1.1, 1.2, 0.8, 0.4],\n",
    "              [0.2, 0.4, 1.0, 0.2],\n",
    "              [0.6, 1.5, 1.2, 0.7]])\n",
    "\n",
    "# A:  A data array with the shape = (n_hidden_units + 1, n_samples)\n",
    "# Note that the first column of this array must be equal to 1.\n",
    "\n",
    "A = np.array([[1.0, 0.1, 0.4, 0.6]])\n",
    "\n",
    "Z = np.dot(W, A[0])\n",
    "y_probas = logistic(Z)\n",
    "print(\"Net Input:\\n{}.\".format(Z))\n",
    "print(\"\\nOutput Units:\\n{}.\".format(y_probas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting values cannot be interpreted as probabilities for a 3-class problem becuase they do not sum up to unity (1.0).  However, this is not a big concern if we **only use our model to predict the class labels**, not the class membership probabilities.  One way to predict the class label from the output units obtained earlier is to use the maximum value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class label is: 0.\n"
     ]
    }
   ],
   "source": [
    "y_class = np.argmax(Z, axis=0)\n",
    "print(\"The predicted class label is: {}.\".format(y_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In certain contexts, however, it can be useful to compute meaningful class probabilities for multiclass predictions.  In the next section, we will take a look at a generalization of the logistic function, the `softmax` function, which can help us with this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating class probabilities in multiclass classification via the softmax function.\n",
    "\n",
    "The `softmax` function is in fact a soft form of the `argmax` function; instead of giving a single class index, it provides the probability of each class.\n",
    "\n",
    "In `softmax`, the probability of a particular sample with net input $z$ belonging to the $i^{\\mathrm{th}}$ can be computed with a normalisation term in the denominator, that is, the sum of all the $M$ linear functions:\n",
    "\n",
    "$$\n",
    "p(y=i|z) = \\phi(z) = \\frac{e^{z_i}}{\\sum_{i=1}^{M}e^{z_j}}\n",
    "$$\n",
    "\n",
    "To see a `softmax` in action, let us code it up in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      " [0.44668973 0.16107406 0.39223621]\n",
      "\n",
      "Sum of all the probabilities:\n",
      "1.0.\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "y_probas = softmax(Z)\n",
    "\n",
    "print(\"Probabilities:\\n\", y_probas)\n",
    "\n",
    "print(\"\\nSum of all the probabilities:\\n{}.\".format(np.sum(y_probas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadening the output spectrum usig a hyperbolic tangent\n",
    "\n",
    "The **hyperbolic tangent** (commonly known as **tanh**) can be interpreted as a rescaled version of the logistic function:\n",
    "\n",
    "$$\n",
    "\\phi_{\\mathrm{tanh}}(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n",
    "$$\n",
    "\n",
    "The advantage of the hyperbolic tangent over the logistic function is that it has a broader output specturm and ranges in the open interval $\\left( -1, +1 \\right)$, which can improve the convergence of the backpropagation algorithm.\n",
    "\n",
    "For an intuitive comparison of the logistic function and the hyperbolic tangent, let us plot the two sigmoid functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tanh(z):\n",
    "    e_p = np.exp(z)\n",
    "    e_m = np.exp(-z)\n",
    "    return (e_p - e_m) / (e_p + e_m)\n",
    "\n",
    "\n",
    "z = np.arange(-5, 5, 0.0005)\n",
    "log_act = 1.0 / (1.0 + np.exp(-z))\n",
    "tanh_act = tanh(z)\n",
    "\n",
    "plt.ylim([-1.5, 1.5])\n",
    "plt.xlabel(\"Net input $z$\")\n",
    "plt.ylabel(\"Activation Function $\\phi(z)$\")\n",
    "for hl in [1, 0.5, 0, -0.5, -1]:\n",
    "    plt.axhline(hl, color=\"black\", linestyle=\":\")\n",
    "plt.plot(z, tanh_act, linewidth=3, linestyle=\"--\", label=\"tanh\")\n",
    "plt.plot(z, log_act, linewidth=3, label='logistic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rectified Linear Unit Activation Function\n",
    "\n",
    "Before we understand ReLU, we should step back and understand the vanishing gradient problem of `tanh` and the logistic activation functions.\n",
    "\n",
    "To understand this problem, let us assume that we initially have the net input $z_1 = 20$, which changes to $z_2 = 25$.  Computing the `tanh` activation, we get $\\phi(z_1) \\approx 1.0$ and $\\phi(z_2)\\approx 1.0$, which shows no change in the output.\n",
    "\n",
    "This means that the derivative of activations with respect to the net input diminishes as $z$ becomes large.  As a result, learning the weights during the training phase becomes very slow because the gradient terms may be very close to zero.  ReLU addresses this issue.  Mathematically, ReLU is defined as follows:\n",
    "\n",
    "$$\n",
    "\\phi(z) = \\mathrm{max}(0, z)\n",
    "$$\n",
    "\n",
    "ReLU is still a non-linear function that is good for learning complex functions with neural networks.  Besides this, the derivative of ReLU with respect to its input is always 1 for positive input values, and therefore it solves the problem of vanishing gradients.\n",
    "\n",
    "Let us conclude this section with an overview of the different activation functions that we encountered in this book:\n",
    "\n",
    "<img src=\"images/13_04.png\" style=\"width:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outtro\n",
    "\n",
    "In the next chapter, we will continue our journey and dive deeper into TensorFlow, and we'll find ourselves working with graph and session objects.  Along the way, we will learn many new concepts, such as placeholders, variables, and saving and restoring models in TensorFlow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Information\n",
    "\n",
    "Since the perceptron rule and Adaline are very similar, we will take the perceptron implementation that we defined earlier and change the ```fit``` method so that the weights are updted by minimising the cost function via gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineGD(object):\n",
    "    \"\"\"\n",
    "    ADAptive LInear NEuron classifier.\n",
    "    \n",
    "    ----------\n",
    "    Parameters\n",
    "    ----------\n",
    "    eta: float\n",
    "        The learning rate (between 0.0 and 1.0)\n",
    "    n_iter: int\n",
    "        The number of passes over the training dataset.\n",
    "    random_state: int\n",
    "        The Random Number Generator seed for random weight initialisation.\n",
    "    \n",
    "    ----------\n",
    "    Attributes\n",
    "    ----------\n",
    "    w_: 1d array\n",
    "        The weights after fitting.\n",
    "    cost_: list\n",
    "        The sum-of-squares cost function value in each epoch (in each pass of n_iter)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def net_input(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the net input\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]*1.0\n",
    "    \n",
    "    def activation(self, X):\n",
    "        \"\"\"\n",
    "        Compute the linear activation function output\n",
    "        \"\"\"\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Return the class label by applying the threshold function\n",
    "        \"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the training data.\n",
    "        \n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like with shape of [n_samples rows, by n_features columns]\n",
    "            X is the Training dataset, where n_samples is the number of flowers,\n",
    "            and n_features is the number of dimensions, or columns.\n",
    "        y: array-like with shape of [n_samples rows].\n",
    "            y is the target values; the true class labels.\n",
    "        \n",
    "        -------\n",
    "        Returns\n",
    "        -------\n",
    "        self: object\n",
    "        \"\"\"\n",
    "        \n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "        \n",
    "        self.cost_ = []\n",
    "        \n",
    "        for _ in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            errors = (y - output)  # the true class label minus the calculated outcome\n",
    "            self.w_[1:] += self.eta * X.T.dot(errors)\n",
    "            self.w_[0] += self.eta * errors.sum() * 1.0\n",
    "            cost = (errors**2).sum() / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments on the above code\n",
    "\n",
    "Instead of updating the weights after evaluating each individual training sample (as in the perceptron), here we calculate the gradient based on the whole training dataset via\n",
    "```python\n",
    "self.eta * errors.sum()\n",
    "```\n",
    "for the bias unit (zeroth-weight), and via\n",
    "```python\n",
    "self.eta * X.T.dot(errors)\n",
    "```\n",
    "for the weights 1 --> m, where ```X.T.dot(errors)``` is a matrix-vector multiplication between our feature matrix and the error vector.\n",
    "\n",
    "It should be noted that the ```activation``` method has no effect in the code since it is simply an identity function.  Here we added the activation function (computed via the ```activation``` method) to illustrate how information flows through a single-layer neural network: features from the input data, net input, activation, and output.  \n",
    "\n",
    "In the next chapter we will learn about a logistic regression classifier that uses a non-identity, nonlinear activation function.  We will see that a logistic regression model is closely related to Adaline with the only difference being its activation and cost function.\n",
    "\n",
    "Now, similar to the previous perceptron implmentation, we collect the cost values in a ```self.cost_``` list to check whether the algorithm converges after a number of epochs.\n",
    "\n",
    "In practice, it often requires some experimentation to find a good learing rate $\\eta$ for optimal convergence.  In the upcoming code, we will choose two different learning rates, $\\eta = 0.1$ and $\\eta = 0.0001$ to start with and plot the cost functions versus the number of epochs to see how well the Adaline implementation learns from the training data.\n",
    "\n",
    "## Note on Hyperparameters\n",
    "The learning rate $\\eta$, as well as the number of epochs ```(n_iter```), are the so-called hyperparameters of the perceptron and Adaline learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us now proceed to plot the cost against the number of epochs for 2 different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3               4\n",
       "145  6.7  3.0  5.2  2.3  Iris-virginica\n",
       "146  6.3  2.5  5.0  1.9  Iris-virginica\n",
       "147  6.5  3.0  5.2  2.0  Iris-virginica\n",
       "148  6.2  3.4  5.4  2.3  Iris-virginica\n",
       "149  5.9  3.0  5.1  1.8  Iris-virginica"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/henri/stuff/machine_learning/sebastian_raschka/henris_coding/chapter_02/iris.data\",\n",
    "                 header=None)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
